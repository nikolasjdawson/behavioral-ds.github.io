@inproceedings{kong2021,
    abstract = {Modeling online discourse dynamics is a core activity in understanding the spread of information, both offline and online, and emergent online behavior. There is currently a disconnect between the practitioners of online social media analysis -- usually social, political and communication scientists -- and the accessibility to tools capable of examining online discussions of users. Here we present evently, a tool for modeling online reshare cascades, and particularly retweet cascades, using self-exciting processes. It provides a comprehensive set of functionalities for processing raw data from Twitter public APIs, modeling the temporal dynamics of processed retweet cascades and characterizing online users with a wide range of diffusion measures. This tool is designed for researchers with a wide range of computer expertise, and it includes tutorials and detailed documentation. We illustrate the usage of evently with an end-to-end analysis of online user behavior on a topical dataset relating to COVID-19. We show that, by characterizing users solely based on how their content spreads online, we can disentangle influential users and online bots.},
    address = {Jerusalem, Israel},
    author = {Kong, Quyu and Ram, Rohit and Rizoiu, Marian-Andrei},
    booktitle = {ACM International Conference on Web Search and Data Mining (WSDM)},
    URL_paper = {https://arxiv.org/abs/2006.06167},
    title = {{evently: A Toolkit for Analyzing Online Users via Reshare Cascade Modeling}},
    year = {2021},
    URL_code = {github.com/behavioral-ds/evently}
}

@inproceedings{Rizoiu2013b,
    abstract = {The objective of the thesis is to explore how complex data can be treated using unsupervised machine learning techniques, in which additional information is injected to guide the exploratory process. Starting from specific problems, our contributions take into account the different dimensions of the complex data: their nature (image, text), the additional information attached to the data (labels, structure, concept ontologies) and the temporal dimension. A special attention is given to data representation and how additional information can be leveraged to improve this representation.},
    address = {Beijing, China},
    author = {Rizoiu, Marian-Andrei},
    booktitle = {International Joint Conference on Artificial Intelligence IJCAI'13},
    URL_paper = {https://www.rizoiu.eu/documents/research/papers/RIZOIU_IJCAI-DC-2013.pdf},
    pages = {3239--3240},
    publisher = {AAAI Press},
    series = {IJCAI '13},
    title = {{Semi-Supervised Structuring of Complex Data}},
    year = {2013},
    URL_slides = {https://www.rizoiu.eu/documents/research/presentations/RIZOIU_IJCAI-DC-2013.pdf}
}

@inproceedings{RIZ12,
    abstract = {In this paper, we propose a new time-aware dissimilarity measure that takes into account the temporal dimension. Observations that are close in the description space, but distant in time are considered as dissimilar. We also propose a method to enforce the segmentation contiguity, by introducing, in the objective function, a penalty term inspired from the Normal Distribution Function. We combine the two propositions into a novel time-driven constrained clustering algorithm, called TDCK-Means, which creates a partition of coherent clusters, both in the multidimensional space and in the temporal space. This algorithm uses soft semi-supervised constraints, to encourage adjacent observations belonging to the same entity to be assigned to the same cluster. We apply our algorithm to a Political Studies dataset in order to detect typical evolution phases. We adapt the Shannon entropy in order to measure the entity contiguity, and we show that our proposition consistently improves temporal cohesion of clusters, without any significant loss in the multidimensional variance.},
    address = {Athens, Greece},
    author = {Rizoiu, Marian-Andrei and Velcin, Julien and Lallich, St{\'{e}}phane},
    booktitle = {2012 IEEE 24th International Conference on Tools with Artificial Intelligence},
    doi = {10.1109/ICTAI.2012.88},
    URL_paper = {https://www.rizoiu.eu/documents/research/papers/RIZOIU_ICTAI-2012-preprint.pdf},
    isbn = {978-1-4799-0227-9},
    issn = {10823409},
    keywords = {contiguity penalty function,contiguity penalty function.,semi-supervised clustering,temporal clustering,temporal-aware dissimilarity measure},
    mendeley-groups = {Aigaion Import},
    month = {nov},
    pages = {610--617},
    publisher = {IEEE},
    series = {ICTAI '12},
    title = {{Structuring Typical Evolutions Using Temporal-Driven Constrained Clustering}},
    url = {http://ieeexplore.ieee.org/document/6495100/},
    volume = {1},
    year = {2012},
    URL_slides = {https://www.rizoiu.eu/documents/research/presentations/RIZOIU_ICTAI-2012-slides.pdf}
}
@article{Musat2012,
    abstract = {This work outlines a novel system that automatically extracts conceptual labels for statistically obtained topics. By creating a projection of the topic, which is a distribution over all the vocabulary words, over the WordNet ontology we succeed in associating concepts to the said groups of words. The most important contributions of this paper are connected to the validation of the role of these concepts as topical labels and the determination of correlations that emerge between the utility of these labels and the strength of the relation between the concepts and the topics.},
    author = {Muşat, Claudiu Cristian and Trǎuşan-Matu, Ştefan and Velcin, Julien and Rizoiu, Marian-Andrei},
    URL_paper = {:home/andrei/Mendeley Desktop/Muşat et al/UPB Scientific Bulletin, Series C Electrical Engineering/Muşat et al. - 2012 - Automatic extraction of conceptual labels from topic models.pdf:pdf},
    journal = {UPB Scientific Bulletin, Series C: Electrical Engineering},
    keywords = {Conceptual processing,Labels,Topic models,WordNet},
    number = {2},
    pages = {57--68},
    title = {{Automatic extraction of conceptual labels from topic models}},
    volume = {74},
    year = {2012}
}
@inproceedings{MUS11,
    abstract = {The growing number of statistical topic models led to the need to better evaluate their output. Traditional evaluation means estimate the model's fitness to unseen data. It has recently been proven than the output of human judgment can greatly differ from these measures. Thus the need for methods that better emulate human judgment is stringent. In this paper we present a system that computes the conceptual relevance of individual topics from a given model on the basis of information drawn from a given concept hierarchy, in this case WordNet. The notion of conceptual relevance is regarded as the ability to attribute a concept to each topic and separate words related to the topic from the unrelated ones based on that concept. In multiple experiments we prove the correlation between the automatic evaluation method and the answers received from human evaluators, for various corpora and difficulty levels. By changing the evaluation focus from a statistical one to a conceptual one we were able to detect which topics are conceptually meaningful and rank them accordingly.},
    address = {Barcelona, Catalonia, Spain},
    author = {Muşat, Claudiu Cristian and Velcin, Julien and Trǎuşan-Matu, Ştefan and Rizoiu, Marian-Andrei},
    booktitle = {International Joint Conference on Artificial Intelligence, Proceedings of the Twenty-Second},
    doi = {10.5591/978-1-57735-516-8/IJCAI11-312},
    isbn = {978-1-57735-515-1},
    keywords = {natural language processing},
    pages = {1866--1871},
    publisher = {AAAI Press},
    series = {IJCAI 2011},
    title = {{Improving topic evaluation using conceptual knowledge}},
    url = {http://www.aaai.org/ocs/index.php/IJCAI/IJCAI11/paper/viewPDFInterstitial/3010/3754},
    volume = {3},
    year = {2011},
    URL_paper = {https://www.rizoiu.eu/documents/research/papers/RIZOIU_IJCAI-2011.pdf},
}
@inproceedings{MUS11a,
    abstract = {We propose a system which employs conceptual knowledge to improve topic models by removing unrelated words from the simplified topic description. We use WordNet to detect which topical words are not conceptually similar to the others and then test our assumptions against human judgment. Results obtained on two different corpora in different test conditions show that the words detected as unrelated had a much greater probability than the others to be chosen by human evaluators as not being part of the topic at all. We prove that there is a strong correlation between the said probability and an automatically calculated topical fitness and we discuss the variation of the correlation depending on the method and data used.},
    address = {Warsaw, Poland},
    author = {Muşat, Claudiu Cristian and Velcin, Julien and Rizoiu, Marian-Andrei and Trǎuşan-Matu, Ştefan},
    booktitle = {International Symposium on Methodologies for Intelligent Systems},
    doi = {10.1007/978-3-642-22732-5_12},
    isbn = {978-3-642-22732-5},
    issn = {1860949X},
    keywords = {Evaluation,Improvement,Ontologies,Topic Models},
    pages = {133--142},
    publisher = {Springer},
    series = {ISMIS{\~{}}'11},
    title = {{Concept-Based Topic Model Improvement}},
    url = {http://link.springer.com/10.1007/978-3-642-22732-5{\_}12},
    volume = {369},
    year = {2011},
    URL_paper = {https://www.rizoiu.eu/documents/research/papers/RIZOIU_ISMIS-2011.pdf},
}
@incollection{Rizoiu2011,
    abstract = {This chapter addresses the issue of topic extraction from text corpora for ontology learning. The first part provides an overview of some of the most significant solutions present today in the literature. These solutions deal mainly with the inferior layers of the Ontology Learning Layer Cake. They are related to the challenges of the Terms and Synonyms layers. The second part shows how these pieces can be bound together into an integrated system for extracting meaningful topics. While the extracted topics are not proper concepts as yet, they constitute a convincing approach towards concept building and therefore ontology learning. This chapter concludes by discussing the research undertaken for filling the gap between topics and concepts as well as perspectives that emerge today in the area of topic extraction.},
    author = {Rizoiu, Marian-Andrei and Velcin, Julien},
    booktitle = {Ontology Learning and Knowledge Discovery Using the Web},
    doi = {10.4018/978-1-60960-625-1.ch003},
    editor = {Wong, Wilson and Liu, Wei and Bennamoun, Mohammed},
    URL_paper = {:home/andrei/Mendeley Desktop/Rizoiu, Velcin/Ontology Learning and Knowledge Discovery Using the Web/Rizoiu, Velcin - 2011 - Topic Extraction for Ontology Learning.pdf:pdf},
    pages = {38--60},
    publisher = {IGI Global},
    title = {{Topic Extraction for Ontology Learning}},
    url = {http://services.igi-global.com/resolvedoi/resolve.aspx?doi=10.4018/978-1-60960-625-1.ch003},
    year = {2011}
}
@inproceedings{RIZ10,
    abstract = {Organiser les donn{\'{e}}es textuelles et en tirer du sens est un d{\'{e}}fi majeur aujourd'hui. Ainsi, lorsque l'on souhaite analyser un d{\'{e}}bat en ligne ou un forum de discussion, on voudrait pouvoir rapidement voir quels sont les principaux th{\`{e}}mes abord{\'{e}}s et la mani{\`{e}}re dont la discussion se structure autour d'eux. Pour cela, et parce que un m{\^{e}}me texte peut {\^{e}}tre associ{\'{e}} {\`{a}} plusieurs th{\`{e}}mes, nous proposons une m{\'{e}}thode originale pour regrouper les donn{\'{e}}es textuelles en autorisant les chevauchements et pour nommer chaque groupe de mani{\`{e}}re lisible. La contribution principale de cet article est une m{\'{e}}thode globale qui permet de r{\'{e}}aliser toute la cha{\^{i}}ne, partant des donn{\'{e}}es textuelles brutes jusqu'{\`{a}} la caract{\'{e}}risation des groupes {\`{a}} un niveau s{\'{e}}mantique qui d{\'{e}}passe le simple ensemble de mots.},
    address = {Hammamet, Tunisie},
    author = {Rizoiu, Marian-Andrei and Velcin, Julien and Chauchat, Jean-Hugues},
    booktitle = {Extraction et Gestion des Connaissances, (EGC 10) 10{\`{e}}me Conf{\'{e}}rence},
    keywords = {clustering,overlapping,text mining},
    organization = {C{\'{e}}padu{\`{e}}s},
    pages = {561--572},
    publisher = {Revue des Nouvelles Technologies de l'Information},
    series = {Revue des Nouvelles Technologies de l'Information},
    title = {{Regrouper les donn{\'{e}}es textuelles et nommer les groupes {\`{a}} l'aide des classes recouvrantes}},
    volume = {E-19},
    year = {2010},
    URL_paper = {https://www.rizoiu.eu/documents/research/papers/RIZOIU_EGC-2010.pdf},
    URL_slides = {https://www.rizoiu.eu/documents/research/presentations/RIZOIU_EGC-2010-slides.pdf}
}

@article{MUS10,
    abstract = {Topic modeling is a growing research field and novel ways of interpreting and evaluating results are necessary. We propose a method for evaluating and improving the performance of topic models generating algorithms relying on WordNet data. We first propose a measure for determining a topic model's fitness factoring in its broadness and redundancy. Then, for each individual topic, the amount of relevant information it provides, along with its most important words and related concepts are determined by defining a cohesion function based on the topic's projection on WordNet concepts. The model as a whole is improved by eliminating each topic's outliers with respect to the ontology projection. We define a inter topic ontology based distance and we further use it to investigate the impact of removing redundant topics from a model with regard to the overlap between topics' ontological projections. Clustering similar topics into conceptually cohesive groups is tried as an alternative to pruning less relevant topics. Results show that evaluating and improving statistical models with WordNet is a promising research track that leads to more coherent topic models.},
    author = {Muşat, Claudiu Cristian and Rizoiu, Marian-Andrei and Trǎuşan-Matu, Ştefan},
    issn = {1843-4460},
    journal = {Romanian Journal of Human-Computer Interaction},
    number = {2},
    pages = {81--96},
    title = {{An Intra and Inter-Topic Evaluation and Cleansing Method}},
    volume = {3},
    year = {2010},
    url = {http://rochi.utcluj.ro/rrioc/en/rrioc-2010-2.html#An_Intra_and_Inter-Topic_Evaluation_and},
    URL_paper = {https://www.rizoiu.eu/documents/research/papers/RIZOIU_RRIOC-2010.pdf}
}
@article{Rizoiu2013a,
    abstract = {Feature-based format is the main data representation format used by machine learning algorithms. When the features do not properly describe the initial data, performance starts to degrade. Some algorithms address this problem by internally changing the representation space, but the newly-constructed features are rarely comprehensible. We seek to construct, in an unsupervised way, new features that are more appropriate for describing a given dataset and, at the same time, comprehensible for a human user. We propose two algorithms that construct the new features as conjunctions of the initial primitive features or their negations. The generated feature sets have reduced correlations between features and succeed in catching some of the hidden relations between individuals in a dataset. For example, a feature like sky ∧ ¬building ∧ panorama would be true for non-urban images and is more informative than simple features expressing the presence or the absence of an object. The notion of Pareto optimality is used to evaluate feature sets and to obtain a balance between total correlation and the complexity of the resulted feature set. Statistical hypothesis testing is used in order to automatically determine the values of the parameters used for constructing a data-dependent feature set. We experimentally show that our approaches achieve the construction of informative feature sets for multiple datasets.},
    author = {Rizoiu, Marian-Andrei and Velcin, Julien and Lallich, St{\'{e}}phane},
    doi = {10.1007/s10844-013-0235-x},
    URL_paper = {http://arxiv.org/pdf/1512.05467.pdf},
    issn = {0925-9902},
    journal = {Journal of Intelligent Information Systems},
    keywords = {Algorithms for data and knowledge management,Clustering,Data mining,Feature evaluation,Heuristic methods,Nonparametric statistics,Pattern analysis,Representations,Unsupervised feature construction,clustering,data mining,feature evaluation,nonparametric statistics,representations,unsupervised feature construction},
    month = {jun},
    number = {3},
    pages = {501--527},
    title = {{Unsupervised feature construction for improving data representation and semantics}},
    url = {http://link.springer.com/10.1007/s10844-013-0235-x},
    volume = {40},
    year = {2013}
}

@article{Rizoiu2014b,
    abstract = {In this paper, we propose a new time-aware dissimilarity measure that takes into account the temporal dimension. Observations that are close in the description space, but distant in time are considered as dissimilar. We also propose a method to enforce the segmentation contiguity, by introducing, in the objective function, a penalty term inspired from the Normal Distribution Function. We combine the two propositions into a novel time-driven constrained clustering algorithm, called TDCK-Means, which creates a partition of coherent clusters, both in the multidimensional space and in the temporal space. This algorithm uses soft semi-supervised constraints, to encourage adjacent observations belonging to the same entity to be assigned to the same cluster. We apply our algorithm to a Political Studies dataset in order to detect typical evolution phases. We adapt the Shannon entropy in order to measure the entity contiguity, and we show that our proposition consistently improves temporal cohesion of clusters, without any significant loss in the multidimensional variance.},
    author = {Rizoiu, Marian-Andrei and Velcin, Julien and Lallich, St{\'{e}}phane},
    doi = {10.1142/S0218213014600136},
    URL_paper = {http://arxiv.org/pdf/1601.02603.pdf},
    issn = {0218-2130},
    journal = {International Journal on Artificial Intelligence Tools},
    month = {aug},
    number = {04},
    pages = {1460013},
    title = {{How to Use Temporal-Driven Constrained Clustering to Detect Typical Evolutions}},
    url = {http://www.worldscientific.com/doi/abs/10.1142/S0218213014600136},
    volume = {23},
    year = {2014}
}
@article{Rizoiu2015a,
    abstract = {One of the prevalent learning tasks involving images is content-based image classification. This is a difficult task especially because the low-level features used to digitally describe images usually capture little information about the semantics of the images. In this paper, we tackle this difficulty by enriching the semantic content of the image representation by using external knowledge. The underlying hypothesis of our work is that creating a more semantically rich representation for images would yield higher machine learning performances, without the need to modify the learning algorithms themselves. The external semantic information is presented under the form of non-positional image labels, therefore positioning our work in a weakly supervised context. Two approaches are proposed: the first one leverages the labels into the visual vocabulary construction algorithm, the result being dedicated visual vocabularies. The second approach adds a filtering phase as a pre-processing of the vocabulary construction. Known positive and known negative sets are constructed and features that are unlikely to be associated with the objects denoted by the labels are filtered. We apply our proposition to the task of content-based image classification and we show that semantically enriching the image representation yields higher classification performances than the baseline representation.},
    author = {Rizoiu, Marian-Andrei and Velcin, Julien and Lallich, St{\'{e}}phane},
    doi = {10.3233/IDA-140702},
    URL_paper = {http://arxiv.org/pdf/1512.04605.pdf},
    journal = {Intelligent Data Analysis},
    keywords = {bag-of-features representation,image numerical representation,semantic-enriched representation,semisupervised learning,visual vocabulary construction},
    number = {1},
    pages = {161--185},
    title = {{Semantic-enriched Visual Vocabulary Construction in a Weakly Supervised Context}},
    volume = {19},
    year = {2015},
    url = {http://dx.doi.org/10.3233/IDA-140702}
}
@article{Rizoiu2015,
    abstract = {We present CommentWatcher, an open source tool aimed at analyzing discussions on web forums. Constructed as a web platform, CommentWatcher features automatic mass fetching of user posts from forum on multiple sites, extracting topics, visualizing the topics as an expression cloud and exploring their temporal evolution. The underlying social network of users is simultaneously constructed using the citation relations between users and visualized as a graph structure. Our platform addresses the issues of the diversity and dynamics of structures of webpages hosting the forums by implementing a parser architecture that is independent of the HTML structure of webpages. This allows easy on-the-fly adding of new websites. Two types of users are targeted: end users who seek to study the discussed topics and their temporal evolution, and researchers in need of establishing a forum benchmark dataset and comparing the performances of analysis tools.},
    annote = {NULL},
    archivePrefix = {arXiv},
    arxivId = {1504.07459},
    author = {Rizoiu, Marian-Andrei and Guille, Adrien and Velcin, Julien},
    eprint = {1504.07459},
    URL_paper = {http://arxiv.org/pdf/1504.07459.pdf},
    journal = {arXiv preprint},
    month = {apr},
    title = {{CommentWatcher: An Open Source Web-based platform for analyzing discussions on web forums}},
    url = {http://arxiv.org/abs/1504.07459},
    year = {2015},
    URL_code = {https://github.com/behavioral-ds/CommentWatcher}
}
@inproceedings{Kim2015a,
    abstract = {Evolutionary clustering aims at capturing the temporal evolution of clusters. This issue is particularly important in the context of social media data that are naturally temporally driven. In this paper, we propose a new probabilistic model-based evolutionary clustering technique. The Temporal Multinomial Mixture (TMM) is an extension of classical mixture model that optimizes feature co-occurrences in the trade-off with temporal smoothness. Our model is evaluated for two recent case studies on opinion aggregation over time. We compare four different probabilistic clustering models and we show the superiority of our proposal in the task of instance-oriented clustering.},
    address = {Vienna, Austria.},
    archivePrefix = {arXiv},
    arxivId = {1601.02300},
    author = {Kim, Young-Min and Velcin, Julien and Bonnevay, St{\'{e}}phane and Rizoiu, Marian-Andrei},
    booktitle = {European Conference on Information Retrieval, Proceedings of the 37th},
    doi = {10.1007/978-3-319-16354-3_66},
    eprint = {1601.02300},
    URL_paper = {http://arxiv.org/pdf/1601.02300.pdf},
    isbn = {9783319163536},
    issn = {16113349},
    keywords = {evolutionary clustering,mixture model,temporal analysis.},
    pages = {593--604},
    publisher = {Springer International Publishing Switzerland},
    series = {ECIR '15},
    title = {{Temporal Multinomial Mixture for Instance-Oriented Evolutionary Clustering}},
    url = {http://link.springer.com/10.1007/978-3-319-16354-3{\_}66},
    volume = {9022},
    year = {2015}
}
@article{Rizoiu2016a,
    abstract = {We propose ClusPath, a novel algorithm for detecting general evolution tendencies in a population of entities. We show how abstract notions, such as the Swedish socio-economical model (in a political dataset) or the companies fiscal optimization (in an eco- nomical dataset) can be inferred from low-level descriptive features. Such high-level regularities in the evolution of entities are detected by combining spatial and temporal features into a spatio-temporal dissimilarity measure and using semi-supervised clustering techniques. The relations between the evolution phases are modeled using a graph structure, inferred si- multaneously with the partition, by using a “slow changing world” assumption. The idea is to ensure a smooth passage for entities along their evolution paths, which catches the long- term trends in the dataset. Additionally, we also provide a method, based on an evolutionary algorithm, to tune the parameters of ClusPath to new, unseen datasets. This method assesses the fitness of a solution using four opposed quality measures and proposes a balanced com- promise.},
    author = {Rizoiu, Marian-Andrei and Velcin, Julien and Bonnevay, St{\'{e}}phane and Lallich, St{\'{e}}phane},
    doi = {10.1007/s10618-015-0445-7},
    URL_paper = {http://arxiv.org/pdf/1512.03501.pdf},
    issn = {1384-5810},
    journal = {Data Mining and Knowledge Discovery},
    keywords = {Pareto front estimation,detection of long-term trends,evolutionary clustering,semi-supervised clustering,temporal cluster graph,temporal clustering},
    month = {sep},
    number = {5},
    pages = {1324--1349},
    series = {EKML/PKDD{\~{}}'14},
    title = {{ClusPath: a temporal-driven clustering to infer typical evolution paths}},
    url = {http://link.springer.com/10.1007/s10618-015-0445-7},
    volume = {30},
    year = {2016},
    URL_slides = {https://www.rizoiu.eu/documents/research/presentations/RIZOIU_PKDD-2016_slides.pdf},
    URL_code = {https://github.com/behavioral-ds/cluspath-distrib}
}


@article{kong2020cikm,
    author={Kong, Quyu and Rizoiu, Marian-Andrei and Xie, Lexing},
    title={Describing and Predicting Online Items with Reshare Cascades via Dual Mixture Self-exciting Processes},
    booktitle={ACM International Conference on Information and Knowledge Management (CIKM'20)},
    pages={645--654},
    year={2020},
    url_Paper = {https://arxiv.org/pdf/2001.11132.pdf},
    url_Code = {https://github.com/qykong/dual-mixture-hawkes-processes},
    abstract = {It is well-known that online behavior is long-tailed, with most cascaded actions being short and a few being very long. A prominent drawback in generative models for online events is the inability to describe unpopular items well. This work addresses these shortcomings by proposing dual mixture self-exciting processes to jointly learn from groups of cascades. We first start from the observation that maximum likelihood estimates for content virality and influence decay are separable in a Hawkes process. Next, our proposed model, which leverages a Borel mixture model and a kernel mixture model, jointly models the unfolding of a heterogeneous set of cascades. When applied to cascades of the same online items, the model directly characterizes their spread dynamics and supplies interpretable quantities, such as content virality and content influence decay, as well as methods for predicting the final content popularities. On two retweet cascade datasets --- one relating to YouTube videos and the second relating to controversial news articles --- we show that our models capture the differences between online items at the granularity of items, publishers and categories. In particular, we are able to distinguish between far-right, conspiracy, controversial and reputable online news articles based on how they diffuse through social media, achieving an F1 score of 0.945. On holdout datasets, we show that the dual mixture model provides, for reshare diffusion cascades especially unpopular ones, better generalization performance and, for online items, accurate item popularity predictions.}
}

@inproceedings{kong2020modeling,
    title={Modeling Information Cascades with Self-exciting Processes via Generalized Epidemic Models},
    author={Kong, Quyu and Rizoiu, Marian-Andrei and Xie, Lexing},
    booktitle={ACM International Conference on Web Search and Data Mining (WSDM)},
    year={2020},
    abstract = {Epidemic models and self-exciting processes are two types of models used to describe diffusion phenomena online and offline. These models were originally developed in different scientific communities, and their commonalities are under-explored. This work establishes, for the first time, a general connection between the two model classes via three new mathematical components. The first is a generalized version of stochastic Susceptible-Infected-Recovered (SIR) model with arbitrary recovery time distributions; the second is the relationship between the (latent and arbitrary) recovery time distribution, recovery hazard function, and the infection kernel of self-exciting processes; the third includes methods for simulating, fitting, evaluating and predicting the generalized process. On three large Twitter diffusion datasets, we conduct goodness-of-fit tests and holdout log-likelihood evaluation of self-exciting processes with three infection kernels --- exponential, power-law and Tsallis Q-exponential. We show that the modeling performance of the infection kernels varies with respect to the temporal structures of diffusions, and also with respect to user behavior, such as the likelihood of being bots. We further improve the prediction of popularity by combining two models that are identified as complementary by the goodness-of-fit tests.},
    url_Paper = {https://arxiv.org/abs/1910.05451},
    url_Code = {https://github.com/qykong/generalized-sir-and-hawkes}
}

@inproceedings{wu2019estimating,
    address = {Austin, TX, USA},
    author = {Wu, Siqi and Rizoiu, Marian-Andrei and Xie, Lexing},
    booktitle = {ACM Conference on Computer-Supported Cooperative Work and Social Computing (CSCW '19)},
    title = {Estimating Attention Flow in Online Video Networks},
    year = {2019},
    abstract = {Online videos have shown tremendous increase in Internet traffic. Most video hosting sites implement recommender systems, which connect the videos into a directed network and conceptually act as a source of pathways for users to navigate. At present, little is known about how human attention is allocated over such large-scale networks, and about the impacts of the recommender systems. In this paper, we first construct the Vevo network -- a YouTube video network with 60,740 music videos interconnected by the recommendation links, and we collect their associated viewing dynamics. This results in a total of 310 million views every day over a period of 9 weeks. Next, we present large-scale measurements that connect the structure of the recommendation network and the video attention dynamics. We use the bow-tie structure to characterize the Vevo network and we find that its core component (23.1% of the videos), which occupies most of the attention (82.6% of the views), is made out of videos that are mainly recommended among themselves. This is indicative of the links between video recommendation and the inequality of attention allocation. Finally, we address the task of estimating the attention flow in the video recommendation network. We propose a model that accounts for the network effects for predicting video popularity, and we show it consistently outperforms the baselines. This model also identifies a group of artists gaining attention because of the recommendation network. Altogether, our observations and our models provide a new set of tools to better understand the impacts of recommender systems on collective social attention.},
    url = {https://dl.acm.org/doi/10.1145/3359285},
    url_Abstract = {https://arxiv.org/abs/1908.07123},
    url_Paper = {https://arxiv.org/pdf/1908.07123.pdf},
    url_Data = {https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/TORICY},
    url_Code = {https://github.com/avalanchesiqi/networked-popularity},
    url_Slides = {http://users.cecs.anu.edu.au/~siqi.wu/files/cscw2019slides.pdf},
    url_Blog = {https://medium.com/acm-cscw/how-does-the-network-of-youtube-music-videos-drive-attention-42130144b59b}
}

@inproceedings{zhang2019efficient,
    author    = {Rui Zhang and Christian Walder and Marian-Andrei Rizoiu and Lexing Xie},
    title     = {Efﬁcient Non-parametric Bayesian Hawkes Processes},
    booktitle={The 28th International Joint Conference on Artificial Intelligence (IJCAI-19)},
    year={2019},
    address = {Macau, China},
    abstract = {In this paper, we develop an efﬁcient non-parametric Bayesian estimation of the kernel function of Hawkes processes. The non-parametric Bayesian approach is important because it provides ﬂexible Hawkes kernels and quantiﬁes their uncertainty. Our method is based on the cluster representation of Hawkes processes. Utilizing the stationarity of the Hawkes process, we efﬁciently sample random branching structures and thus, we split the Hawkes process into clusters of Poisson processes. We derive two algorithms — a block Gibbs sampler and a maximum a posteriori estimator based on expectation maximization — and we show that our methods have a linear time complexity, both theoretically and empirically. On synthetic data, we show our methods to be able to infer ﬂexible Hawkes triggering kernels. On two large scale Twitter diffusion datasets, we show that our methods outperform the current state-of-the-art in goodness-of-ﬁt and that the time complexity is linear in the size of the dataset. We also observe that on diffusions related to online videos, the learned kernels reﬂect the perceived longevity for different content types such as music or pets videos.},
    url_Paper = {https://arxiv.org/abs/1810.03730},
    url_Code = {https://github.com/RuiZhang2016/Efficient-Nonparametric-Bayesian-Hawkes-Processes}
}

@Article{Kim2019,
    author={Kim, Dongwoo and Graham, Timothy and Wan, Zimin and Rizoiu, Marian-Andrei},
    title={Analysing user identity via time-sensitive semantic edit distance (t-SED): a case study of Russian trolls on Twitter},
    journal={Journal of Computational Social Science},
    year={2019},
    month={Jul},
    day={01},
    volume={2},
    number={2},
    pages={331--351},
    abstract={In the digital era, individuals are increasingly profiled and grouped based on the traces that they leave behind in online social networks such as Twitter and Facebook. In this paper, we develop and evaluate a novel text analysis approach for studying user identity and social roles by redefining identity as a sequence of timestamped items (e.g., tweet texts). We operationalise this idea by developing a novel text distance metric, the time-sensitive semantic edit distance (t-SED), which accounts for the temporal context across multiple traces. To evaluate this method, we undertake a case study of Russian online-troll activity within US political discourse. The novel metric allows us to classify the social roles of trolls based on their traces, in this case tweets, into one of the predefined categories left-leaning, right-leaning, and news feed. We show the effectiveness of the t-SED metric to measure the similarities between tweets while accounting for the temporal context, and we use novel data visualisation techniques and qualitative analysis to uncover new empirical insights into Russian troll activity that have not been identified in the previous work. In addition, we highlight a connection with the field of actor--network theory and the related hypotheses of Gabriel Tarde, and we discuss how social sequence analysis using t-SED may provide new avenues for tackling a longstanding problem in social theory: how to analyse society without separating reality into micro vs. macro-levels.},
    issn={2432-2725},
    doi={10.1007/s42001-019-00051-x},
    url={https://doi.org/10.1007/s42001-019-00051-x},
    url_paper = {https://arxiv.org/pdf/1901.05228.pdf}
}

@inproceedings{kong2019linking,
    address = {Melbourne, VIC, Australia},
    author = {Kong, Quyu},
    booktitle = {ACM International Conference on Web Search and Data Mining (WSDM '19), Doctoral Consortium},
    title = {{Linking Epidemic Models and Hawkes Point Processes for Modeling Information Diffusion}},
    year = {2019},
    abstract = {Epidemic models and Hawkes point process models are two common model classes for information diffusion. Recent work has revealed the equivalence between the two for information diffusion modeling. This allows tools created for one class of models to be applied to another. However, epidemic models and Hawkes point processes can be connected in more ways. This thesis aims to develop a rich set of mathematical equivalences and extensions, and use them to ask and answer questions in social media and beyond. Specifically, we show our plan of generalizing the equivalence of the two model classes by extending it to Hawkes point process models with arbitrary memory kernels. We then outline a rich set of quantities describing diffusion, including diffusion size and extinction probability, introduced in the fields where the models are originally designed. Lastly, we discuss some novel applications of these quantities in a range of problems such as popularity prediction and popularity intervention.},
    url_Paper = {http://cm.cecs.anu.edu.au/documents/kong_wsdm2019_dc.pdf}
}

@inproceedings{Rizoiu2018a,
    abstract = {Serious concerns have been raised about the role of `socialbots' in manipulating public opinion and influencing the outcome of elections by retweeting partisan content to increase its reach. Here we analyze the role and influence of socialbots on Twitter by determining how they contribute to retweet diffusions. We collect a large dataset of tweets during the 1st U.S. presidential debate in 2016 and we analyze its 1.5 million users from three perspectives: user influence, political behavior (partisanship and engagement) and botness. First, we define a measure of user influence based on the user's active contributions to information diffusions, i.e. their tweets and retweets. Given that Twitter does not expose the retweet structure -- it associates all retweets with the original tweet -- we model the latent diffusion structure using only tweet time and user features, and we implement a scalable novel approach to estimate influence over all possible unfoldings. Next, we use partisan hashtag analysis to quantify user political polarization and engagement. Finally, we use the BotOrNot API to measure user botness (the likelihood of being a bot). We build a two-dimensional "polarization map" that allows for a nuanced analysis of the interplay between botness, partisanship and influence. We find that not only are socialbots more active on Twitter -- starting more retweet cascades and retweeting more -- but they are 2.5 times more influential than humans, and more politically engaged. Moreover, pro-Republican bots are both more influential and more politically engaged than their pro-Democrat counterparts. However we caution against blanket statements that software designed to appear human dominates politics-related activity on Twitter. Firstly, it is known that accounts controlled by teams of humans (e.g. organizational accounts) are often identified as bots. Secondly, we find that many highly influential Twitter users are in fact pro-Democrat and that most pro-Republican users are mid-influential and likely to be human (low botness).},
    address = {Stanford, CA, USA},
    author = {Rizoiu, Marian-Andrei and Graham, Timothy and Zhang, Rui and Zhang, Yifei and Ackland, Robert and Xie, Lexing},
    booktitle = {International AAAI Conference on Web and Social Media (ICWSM '18)},
    title = {{{\#}DebateNight: The Role and Influence of Socialbots on Twitter During the 1st 2016 U.S. Presidential Debate}},
    year = {2018},
    url_Abstract = {https://arxiv.org/abs/1802.09808},
    url_Paper = {https://arxiv.org/pdf/1802.09808.pdf},
    url_Code = {https://github.com/computationalmedia/cascade-influence},
    url_Slides = {http://www.rizoiu.eu/documents/research/presentations/RIZOIU_ICWSM-2018_slides.pdf}
}

@inproceedings{wu2018beyond,
    address = {Stanford, CA, USA},
    author = {Wu, Siqi and Rizoiu, Marian-Andrei and Xie, Lexing},
    booktitle = {International AAAI Conference on Web and Social Media (ICWSM '18)},
    title = {Beyond Views: Measuring and Predicting Engagement in Online Videos},
    year = {2018},
    abstract = {The share of videos in the internet traffic has been growing, therefore understanding how videos capture attention on a global scale is also of growing importance. Most current research focus on modeling the number of views, but we argue that video engagement, or time spent watching is a more appropriate measure for resource allocation problems in attention, networking, and promotion activities. In this paper, we present a first large-scale measurement of video-level aggregate engagement from publicly available data streams, on a collection of 5.3 million YouTube videos published over two months in 2016. We study a set of metrics including time and the average percentage of a video watched. We define a new metric, relative engagement, that is calibrated against video properties and strongly correlate with recognized notions of quality. Moreover, we find that engagement measures of a video are stable over time, thus separating the concerns for modeling engagement and those for popularity -- the latter is known to be unstable over time and driven by external promotions. We also find engagement metrics predictable from a cold-start setup, having most of its variance explained by video context, topics and channel information -- R2=0.77. Our observations imply several prospective uses of engagement metrics -- choosing engaging topics for video production, or promoting engaging videos in recommender systems.},
    url_Abstract = {https://arxiv.org/abs/1709.02541},
    url_Paper = {https://arxiv.org/pdf/1709.02541.pdf},
    url_Code = {https://github.com/avalanchesiqi/youtube-engagement},
    url_Slides = {http://cm.cecs.anu.edu.au/documents/wu_icwsm2018_slides.pdf}
}

@inproceedings{Mishra2018rnn-mas,
    address = {Stanford, CA, USA},
    author = {Mishra, Swapnil and Rizoiu, Marian-Andrei and Xie, Lexing},
    booktitle = {International AAAI Conference on Web and Social Media (ICWSM '18)},
    pages = {1--10},
    title = {{Modeling Popularity in Asynchronous Social Media Streams with Recurrent Neural Networks}},
    year = {2018},
    abstract = {Understanding and predicting the popularity of online items is an important open problem in social media analysis. Considerable progress has been made recently in data-driven predictions, and in linking popularity to external promotions. However, the existing methods typically focus on a single source of external influence, whereas for many types of online content such as YouTube videos or news articles, attention is driven by multiple heterogeneous sources simultaneously - e.g. microblogs or traditional media coverage. Here, we propose RNN-MAS, a recurrent neural network for modeling asynchronous streams. It is a sequence generator that connects multiple streams of different granularity via joint inference. We show RNN-MAS not only to outperform the current state-of-the-art Youtube popularity prediction system by 17%, but also to capture complex dynamics, such as seasonal trends of unseen influence. We define two new metrics: promotion score quantifies the gain in popularity from one unit of promotion for a Youtube video; the loudness level captures the effects of a particular user tweeting about the video. We use the loudness level to compare the effects of a video being promoted by a single highly-followed user (in the top 1% most followed users) against being promoted by a group of mid-followed users. We find that results depend on the type of content being promoted: superusers are more successful in promoting Howto and Gaming videos, whereas the cohort of regular users are more influential for Activism videos. This work provides more accurate and explainable popularity predictions, as well as computational tools for content producers and marketers to allocate resources for promotion campaigns.},
    url_Abstract = {https://arxiv.org/abs/1804.02101},
    url_Paper = {https://arxiv.org/pdf/1804.02101.pdf},
    url_Code = {https://github.com/computationalmedia/rnn-mas}
}

@incollection{Rizoiu2018HPE,
    author = {Rizoiu, Marian-Andrei and Lee, Young and Mishra, Swapnil and Xie, Lexing},
    title = {{A Tutorial on Hawkes Processes for Events in Social Media}},
    booktitle = {Frontiers of Multimedia Research},
    editor = {Chang, Shih-Fu},
    year = {2018},
    isbn = {978-1-97000-107-5},
    pages = {191--218},
    numpages = {28},
    abstract = {This chapter provides an accessible introduction for point processes, and especially Hawkes processes, for modeling discrete, inter-dependent events over continuous time. We start by reviewing the definitions and the key concepts in point processes. We then introduce the Hawkes process, its event intensity function, as well as schemes for event simulation and parameter estimation. We also describe a practical example drawn from social media data - we show how to model retweet cascades using a Hawkes self-exciting process. We presents a design of the memory kernel, and results on estimating parameters and predicting popularity. The code and sample event data are available as an online appendix.},
    url = {https://doi.org/10.1145/3122865.3122874},
    url_Abstract = {https://arxiv.org/abs/1708.06401},
    url_Paper = {https://arxiv.org/pdf/1708.06401.pdf},
    url_Code= {https://github.com/s-mishra/featuredriven-hawkes},
    doi = {10.1145/3122865.3122874},
    acmid = {3122874},
    publisher = {Association for Computing Machinery and Morgan \& Claypool},
    address = {New York, NY, USA}
}

@inproceedings{rizoiu2018sir,
    title={{SIR-Hawkes}: Linking Epidemic Models and {Hawkes} Processes to Model Diffusions in Finite Populations},
    author={Rizoiu, Marian-Andrei and Mishra, Swapnil and Kong, Quyu and Carman, Mark and Xie, Lexing},
    address = {Lyon, France},
    booktitle = {Proceedings of the 2018 World Wide Web Conference},
    series = {WWW '18},
    year={2018},
    abstract = {Among the statistical tools for online information diffusion modeling, both epidemic models and Hawkes point processes are popular choices. The former originate from epidemiology, and consider information as a viral contagion which spreads into a population of online users. The latter have roots in geophysics and finance, view individual actions as discrete events in continuous time, and modulate the rate of events according to the self-exciting nature of event sequences. Here, we establish a novel connection between these two frameworks. Namely, the rate of events in an extended Hawkes model is identical to the rate of new infections in the Susceptible-Infected-Recovered (SIR) model after marginalizing out recovery events -- which are unobserved in a Hawkes process. This result paves the way to apply tools developed for SIR to Hawkes, and vice versa. It also leads to HawkesN, a generalization of the Hawkes model which accounts for a finite population size. Finally, we derive the distribution of cascade sizes for HawkesN, inspired by methods in stochastic SIR. Such distributions provide nuanced explanations to the general unpredictability of popularity: the distribution for diffusion cascade sizes tends to have two modes, one corresponding to large cascade sizes and another one around zero.},
    doi = {10.1145/3178876.3186108},
    eprint = {1711.01679},
    isbn = {9781450356398},
    pages = {419--428},
    url_Abstract = {https://arxiv.org/abs/1711.01679},
    url_Paper = {https://arxiv.org/pdf/1711.01679.pdf},
    url_Code = {https://github.com/computationalmedia/sir-hawkes},
    url_Slides = {http://www.rizoiu.eu/documents/research/presentations/RIZOIU_WWW-2018_slides.pdf}
}

@inproceedings{kong2018will,
    title={Will This Video Go Viral? Explaining and Predicting the Popularity of Youtube Videos},
    author={Kong, Quyu and Rizoiu, Marian-Andrei and Wu, Siqi and Xie, Lexing},
    address = {Lyon, France},
    booktitle = {Companion Proceedings of the The Web Conference 2018 - Demos},
    series = {WWW '18},
    year={2018},
    abstract = {What makes content go viral? Which videos become popular and why others don't? Such questions have elicited significant attention from both researchers and industry, particularly in the context of online media. A range of models have been recently proposed to explain and predict popularity; however, there is a short supply of practical tools, accessible for regular users, that leverage these theoretical results. Hipie -- an interactive visualization system -- is created to fill this gap, by enabling users to reason about the virality and the popularity of online videos. It retrieves the metadata and the past popularity series of Youtube videos, it employs the Hawkes Intensity Process, a state-of-the-art online popularity model for explaining and predicting video popularity, and it presents videos comparatively in a series of interactive plots. This system will help both content consumers and content producers in a range of data-driven inquiries, such as to comparatively analyze videos and channels, to explain and to predict future popularity, to identify viral videos, and to estimate responses to online promotion. },
    doi = {10.1145/3184558.3186972},
    url_Abstract = {https://arxiv.org/abs/1801.04117},
    url_Paper = {https://arxiv.org/pdf/1801.04117.pdf},
    url_Code = {https://github.com/computationalmedia/hipie}
}

@inproceedings{Rizoiu2017HIP,
    address = {Perth, Australia},
    author = {Rizoiu, Marian-Andrei and Xie, Lexing and Sanner, Scott and Cebrian, Manuel and Yu, Honglin and {Van Hentenryck}, Pascal},
    booktitle = {World Wide Web 2017, International Conference on},
    pages = {735--744},
    title = {Expecting to be {HIP}: Hawkes Intensity Processes for Social Media Popularity},
    year = {2017},
    doi = {10.1145/3038912.3052650},
    isbn = {9781450349130},
    abstract = {Modeling and predicting the popularity of online content is a significant problem for the practice of information dissemination, advertising, and consumption. Recent work analyzing massive datasets advances our understanding of popularity, but one major gap remains: To precisely quantify the relationship between the popularity of an online item and the external promotions it receives. This work supplies the missing link between exogenous inputs from public social media platforms, such as Twitter, and endogenous responses within the content platform, such as YouTube. We develop a novel mathematical model, the Hawkes intensity process, which can explain the complex popularity history of each video according to its type of content, network of diffusion, and sensitivity to promotion. Our model supplies a prototypical description of videos, called an endo-exo map. This map explains popularity as the result of an extrinsic factor -- the amount of promotions from the outside world that the video receives, acting upon two intrinsic factors -- sensitivity to promotion, and inherent virality. We use this model to forecast future popularity given promotions on a large 5-months feed of the most-tweeted videos, and found it to lower the average error by 28.6% from approaches based on popularity history. Finally, we can identify videos that have a high potential to become viral, as well as those for which promotions will have hardly any effect.},
    url_Abstract={https://arxiv.org/abs/1602.06033},
    url_Paper = {https://arxiv.org/pdf/1602.06033},
    url_slides = {https://www.rizoiu.eu/documents/research/presentations/RIZOIU_WWW-2017_slides.pdf},
    url_code = {https://github.com/andrei-rizoiu/hip-popularity}
}

@inproceedings{Rizoiu2017promo,
    address = {Montreal, Canada},
    author = {Rizoiu, Marian-Andrei and Xie, Lexing},
    booktitle = {The International AAAI Conference on Web and Social Media (ICWSM)},
    pages = {182--191},
    title = {Online Popularity under Promotion: Viral Potential, Forecasting, and the Economics of Time},
    year = {2017},
    abstract = {Modeling the popularity dynamics of an online item is an important open problem in computational social science. This paper presents an in-depth study of popularity dynamics under external promotions, especially in predicting popularity jumps of online videos, and determining effective and efficient schedules to promote online content. The recently-proposed Hawkes Intensity Process (HIP) models popularity as a non-linear interplay between exogenous stimuli and the endogenous reaction. We propose two novel metrics based on HIP: to describe popularity gain per unit of promotion, and to quantify the time it takes for such effects to unfold. We make increasingly accurate forecasts of future popularity by including information about the intrinsic properties of the video, promotions it receives, and the non-linear effects of popularity ranking. We illustrate by simulation the interplay between the unfolding of popularity over time, and the time-sensitive value of resources. Lastly, our model lends a novel explanation of the commonly adopted periodic and constant promotion strategy in advertising, as increasing the perceived viral potential. This study provides quantitative guidelines about setting promotion schedules considering content virality, timing, and economics.},
    url_Abstract={https://arxiv.org/abs/1703.01012},
    url_Paper = {https://arxiv.org/pdf/1703.01012},
    url_code = {https://github.com/andrei-rizoiu/hip-popularity},
    url_slides = {https://www.rizoiu.eu/documents/research/presentations/RIZOIU_ICWSM-2017_slides.pdf}
}

@inproceedings{Mishra2016,
    title = {{Feature Driven and Point Process Approaches for Popularity Prediction}},
    author = {Mishra, Swapnil and Rizoiu, Marian-Andrei and Xie, Lexing},
    booktitle = {Proceedings of the 25th ACM International Conference on Information and Knowledge Management},
    series = {CIKM '16},
    address = {Indianapolis, IN, USA},
    doi = {10.1145/2983323.2983812},
    keywords = {social media; self-exciting point process; information diffusion; cascade prediction},
    year = {2016},
    abstract = {Predicting popularity, or the total volume of information outbreaks, is an important subproblem for understanding collective behavior in networks. Each of the two main types of recent approaches to the problem, feature-driven and generative models, have desired qualities and clear limitations. This paper bridges the gap between these solutions with a new hybrid approach and a new performance benchmark. We model each social cascade with a marked Hawkes self-exciting point process, and estimate the content virality, memory decay, and user influence. We then learn a predictive layer for popularity prediction using a collection of cascade history. To our surprise, Hawkes process with a predictive overlay outperform recent feature-driven and generative approaches on existing tweet data [44] and a new public benchmark on news tweets. We also found that a basic set of user features and event time summary statistics performs competitively in both classification and regression tasks, and that adding point process information to the feature set further improves predictions. From these observations, we argue that future work on popularity prediction should compare across feature-driven and generative modeling approaches in both classification and regression tasks.},
    url_Abstract={https://arxiv.org/abs/1608.04862},
    url_Paper = {https://arxiv.org/pdf/1608.04862.pdf},
    url_Presentation_Page = {http://cm.cecs.anu.edu.au/post/fdhawkesforpopularity/},
    url_Slides = {http://cm.cecs.anu.edu.au/documents/smishra_cikm16_presentation.pdf},
    url_code = {https://git.io/v6rIN}
}

@inproceedings{Rizoiu2016,
    abstract = {The cumulative effect of collective participation online has an important and adverse impact on individual privacy. As an online system evolves over time, new digital traces of individual behavior may uncover previously hidden statistical links between an individual’s past actions and her private traits. Furthermore, this de-anonymization trend may not be observable when analyzing short or medium time-span snapshots of data. To quantify this effect, we analyze the evolution of individual privacy loss by studying the 13-year long edit history of Wikipedia, including more than 117,523 different users performing 188,805,088 edits. We trace each Wikipedia’s contributor using apparently harmless features, such as the number of edits performed on predefined broad categories in a given time period (e.g. Mathematics, Culture or Nature). We show that even at this unspecific level of identification, it is possible to use off-the-shelf machine learning algorithms to uncover usually undisclosed private traits, such as gender, religion or education. We provide empirical evidence that the prediction accuracy for almost all private traits consistently improves over time. Moreover, we observe that the system also shows improved prediction for users who participated in the system during “safe” periods — periods where a given individual’s private traits could not be — showing that de-anonymization threats are hard to foresee as online systems evolve. Insights from this work should help users, system designers, and policy makers understand and debate the design and long-term effects of online content creation systems.},
    address = {San Francisco, CA, USA},
    author = {Rizoiu, Marian-Andrei and Xie, Lexing and Caetano, Tiberio and Cebrian, Manuel},
    booktitle = {Proceedings of the 9th ACM International Conference on Web Search and Data Mining},
    series = {WSDM '16},
    doi = {10.1145/2835776.2835798},
    keywords = {de-anonymization,online privacy,temporal loss of privacy},
    title = {{Evolution of Privacy Loss on Wikipedia}},
    year = {2016},
    url_Slides = {https://www.rizoiu.eu/documents/research/presentations/RIZOIU_WSDM-2016_slides.pdf}, % use this to distribute the slides of the presentation
    %url_Link = {http://my.repo.com/awesome.paper.pdf} % use this to distribute the link to your paper (e.g. journal entry)
    url_Paper = {http://arxiv.org/pdf/1512.03523.pdf}, % use this to distribute the link ot the paper. It can be either a http:// link or a local link relative to the bib file (somthing like publications/paper.pdf)
    url_Presentation_Page = {http://cm.cecs.anu.edu.au/post/wikiprivacy/}
}

@inproceedings{Mihaita2019,
    abstract = {Congestion prediction represents a major priority for traffic management centres around the world to ensure timely incident response handling. The increasing amounts of generated traffic data have been used to train machine learning predictors for traffic, however this is a challenging task due to inter-dependencies of traffic flow both in time and space. Recently, deep learning techniques have shown significant prediction improvements over traditional models, however open questions remain around their applicability, accuracy and parameter tuning. This paper proposes an advanced deep learning framework for simultaneously predicting the traffic flow on a large number of monitoring stations along a highly circulated motorway in Sydney, Australia, including exit and entry loop count stations, and over varying training and prediction time horizons. The spatial and temporal features extracted from the 36.34 million data points are used in various deep learning architectures that exploit their spatial structure (convolutional neuronal networks), their temporal dynamics (recurrent neuronal networks), or both through a hybrid spatio-temporal modelling (CNN-LSTM). We show that our deep learning models consistently outperform traditional methods, and we conduct a comparative analysis of the optimal time horizon of historical data required to predict traffic flow at different time points in the future.},
    address = {Auckland, New Zealand},
    archivePrefix = {arXiv},
    arxivId = {1907.06356},
    author = {Mihaita, Adriana-Simona and Li, Haowen and He, Zongyang and Rizoiu, Marian-Andrei},
    booktitle = {2019 IEEE Intelligent Transportation Systems Conference (ITSC)},
    doi = {10.1109/ITSC.2019.8916852},
    eprint = {1907.06356},
    URL_paper = {https://arxiv.org/pdf/1907.06356.pdf},
    isbn = {978-1-5386-7024-8},
    keywords = {,BPNN,CNN,Deep learning,LSTM,Motorway flow predicting,Short- versus long-term prediction},
    month = {oct},
    pages = {1683--1690},
    publisher = {IEEE},
    title = {{Motorway Traffic Flow Prediction using Advanced Deep Learning}},
    url = {https://ieeexplore.ieee.org/document/8916852/},
    year = {2019}
}
@article{Kern2020,
    abstract = {Work is thought to be more enjoyable and beneficial to individuals and society when there is congruence between one's personality and one's occupation. We provide large-scale evidence that occupations have distinctive psychological profiles, which can successfully be predicted from linguistic information unobtrusively collected through social media. Based on 128,279 Twitter users representing 3,513 occupations, we automatically assess user personalities and visually map the personality profiles of different professions. Similar occupations cluster together, pointing to specific sets of jobs that one might be well suited for. Observations that contradict existing classifications may point to emerging occupations relevant to the 21st century workplace. Findings illustrate how social media can be used to match people to their ideal occupation.},
    author = {Kern, Margaret L. and McCarthy, Paul X. and Chakrabarty, Deepanjan and Rizoiu, Marian-Andrei},
    doi = {10.1073/pnas.1917942116},
    URL_paper = {https://www.pnas.org/content/pnas/116/52/26459.full.pdf},
    issn = {0027-8424},
    journal = {Proceedings of the National Academy of Sciences},
    month = {dec},
    number = {52},
    pages = {26459--26464},
    title = {{Social media-predicted personality traits and values can help match people to their ideal jobs}},
    url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1917942116},
    volume = {116},
    year = {2019},
    URL_code = {https://github.com/behavioral-ds/VocationMap}
}
@inproceedings{Mihaita2019a,
    abstract = {Predicting traffic incident duration is a major challenge for many traffic centres around the world. Most research studies focus on predicting the incident duration on motorways rather than arterial roads, due to a high network complexity and lack of data. In this paper we propose a bi-level framework for predicting the accident duration on arterial road networks in Sydney, based on operational requirements of incident clearance target which is less than 45 minutes. Using incident baseline information, we first deploy a classification method using various ensemble tree models in order to predict whether a new incident will be cleared in less than 45min or not. If the incident was classified as short-term, then various regression models are developed for predicting the actual incident duration in minutes by incorporating various traffic flow features. After outlier removal and intensive model hyper-parameter tuning through randomized search and cross-validation, we show that the extreme gradient boost approach outperformed all models, including the gradient-boosted decision-trees by almost 53{\%}. Finally, we perform a feature importance evaluation for incident duration prediction and show that the best prediction results are obtained when leveraging the real-time traffic flow in vicinity road sections to the reported accident location.},
    address = {Singapore},
    archivePrefix = {arXiv},
    arxivId = {1905.12254},
    author = {Mihaita, Adriana-Simona and Liu, Zheyuan and Cai, Chen and Rizoiu, Marian-Andrei},
    booktitle = {Proceedings of the 26th ITS World Congress},
    eprint = {1905.12254},
    URL_paper = {http://arxiv.org/pdf/1905.12254.pdf},
    month = {may},
    pages = {1--12},
    title = {{Arterial incident duration prediction using a bi-level framework of extreme gradient-tree boosting}},
    url = {http://arxiv.org/abs/1905.12254},
    year = {2019}
}
@inproceedings{Ram2019,
    address = {Adelaide, Australia},
    author = {Ram, Rohit and Rizoiu, Marian-Andrei},
    booktitle = {Australian Social Network Analysis Conference (ASNAC'19)},
    URL_paper = {https://opus.lib.uts.edu.au/bitstream/10453/137931/1/main.pdf},
    pages = {2},
    title = {{A social science-grounded approach for quantifying online social influence}},
    year = {2019}
}

@inproceedings{Dawson2019,
    abstract = {Labour demand and skill shortages have historically been difficult to assess given the high costs of conducting representative surveys and the inherent delays of these indicators. This is particularly consequential for fast developing skills and occupations, such as those relating to Data Science and Analytics (DSA). This paper develops a data-driven solution to detecting skill shortages from online job advertisements (ads) data. We first propose a method to generate sets of highly similar skills based on a set of seed skills from job ads. This provides researchers with a novel method to adaptively select occupations based on granular skills data. Next, we apply this adaptive skills similarity technique to a dataset of over 6.7 million Australian job ads in order to identify occupations with the highest proportions of DSA skills. This uncovers 306,577 DSA job ads across 23 occupational classes from 2012-2019. Finally, we propose five variables for detecting skill shortages from online job ads: (1) posting frequency; (2) salary levels; (3) education requirements; (4) experience demands; and (5) job ad posting predictability. This contributes further evidence to the goal of detecting skills shortages in real-time. In conducting this analysis, we also find strong evidence of skills shortages in Australia for highly technical DSA skills and occupations. These results provide insights to Data Science researchers, educators, and policy-makers from other advanced economies about the types of skills that should be cultivated to meet growing DSA labour demands in the future.},
    address = {Los Angeles, CA, USA},
    archivePrefix = {arXiv},
    arxivId = {1911.02302},
    author = {Dawson, Nikolas and Rizoiu, Marian-Andrei and Johnston, Benjamin and Williams, Mary-Anne},
    booktitle = {2019 IEEE International Conference on Big Data (Big Data)},
    doi = {10.1109/BigData47090.2019.9005967},
    eprint = {1911.02302},
    URL_paper = {http://arxiv.org/pdf/1911.02302.pdf},
    isbn = {978-1-7281-0858-2},
    keywords = {Big Data,Data Science,Labour Demand,Online Job Advertisements,Skill Shortages},
    month = {dec},
    pages = {1637--1643},
    publisher = {IEEE},
    title = {{Adaptively selecting occupations to detect skill shortages from online job ads}},
    url = {https://ieeexplore.ieee.org/document/9005967/},
    year = {2019}
}
@inproceedings{Zhang2020,
    abstract = {The Hawkes process has been widely applied to modeling self-exciting events, including neuron spikes, earthquakes and tweets. To avoid designing parametric kernel functions and to be able to quantify the prediction confidence, non-parametric Bayesian Hawkes processes have been proposed. However the inference of such models suffers from unscalability or slow convergence. In this paper, we first propose a new non-parametric Bayesian Hawkes process whose triggering kernel is modeled as a squared sparse Gaussian process. Second, we present the variational inference scheme for the model optimization, which has the advantage of linear time complexity by leveraging the stationarity of the triggering kernel. Third, we contribute a tighter lower bound than the evidence lower bound of the marginal likelihood for the model selection. Finally, we exploit synthetic data and large-scale social media data to validate the efficiency of our method and the practical utility of our approximate marginal likelihood. We show that our approach outperforms state-of-the-art non-parametric Bayesian and non-Bayesian methods.},
    address = {New York, New York, USA},
    annote = {(CoRE: A*, H5: , a.r.: )},
    archivePrefix = {arXiv},
    arxivId = {1905.10496},
    author = {Zhang, Rui and Walder, Christian and Rizoiu, Marian-Andrei},
    booktitle = {AAAI Conference on Artificial Intelligence (AAAI'20)},
    eprint = {1905.10496},
    URL_paper = {http://arxiv.org/pdf/1905.10496.pdf},
    month = {may},
    title = {{Variational Inference for Sparse Gaussian Process Modulated Hawkes Process}},
    url = {https://doi.org/10.1609/aaai.v34i04.6160},
    year = {2020}
}
@inproceedings{Zhang2020a,
    abstract = {Approximate inference techniques are a cornerstone of the study of Gaussian Processes. Despite this, most work approximately optimises divergence measures (Kullback-Leibler (KL), $\alpha$-divergence, etc.) which lack the basic desiderata for the task at hand, while chiefly offering merely technical convenience. We develop a new approximate inference method for the Gaussian process which overcomes the technical challenges of abandoning these convenient divergences. Our method — dubbed Quantile Propagation (QP) — is similar to expectation propagation (EP) but minimizes the L 2 Wasserstein distance (WD). The WD exhibits all the required properties of a distance metric, while respecting the geometry of the underlying sample space. We show that QP matches quantile functions rather than moments as in EP and has the same mean update but a smaller variance update than EP, thereby alleviating the over-estimation of the posterior variance exhibited by EP. Crucially, despite the significant complexity in even evaluating the WD, our QP has the same favorable locality property as EP, and thereby admits an efficient algorithm. Experiments on classification and Poisson regression tasks demonstrate that QP outperforms both EP and variational Bayes.},
    author = {Zhang, Rui and Walder, Christian and Bonilla, Edwin V. and Rizoiu, Marian-Andrei and Xie, Lexing},
    booktitle = {Conference on Neural Information Processing Systems (NeurIPS'20)},
    title = {{Quantile Propagation for Wasserstein-Approximate Gaussian Processes}},
    url = {https://proceedings.neurips.cc//paper/2020/hash/f5e62af885293cf4d511ceef31e61c80-Abstract.html},
    year = {2020},
    url_paper = {https://papers.nips.cc/paper/2020/file/f5e62af885293cf4d511ceef31e61c80-Paper.pdf},
    url_code = {https://github.com/RuiZhang2016/Quantile-Propagation-for-Wasserstein-Approximate-Gaussian-Processes}
}
@inproceedings{Mihaita2020,
    abstract = {Traffic flow prediction, particularly in areas that experience highly dynamic flows such as motorways, is a major issue faced in traffic management. Due to increasingly large volumes of data sets being generated every minute, deep learning methods have been used extensively in the latest years for both short and long term prediction. However, such models, despite their efficiency, need large amounts of historical information to be provided, and they take a considerable amount of time and computing resources to train, validate and test. This paper presents two new spatial-temporal approaches for building accurate short-term prediction along a popular motorway in Sydney, by making use of the graph structure of the motorway network (including exits and entries). The methods are built on proximity-based approaches, denoted backtracking and interpolation, which uses the most recent and closest traffic flow information for each of the target counting stations along the motorway. The results indicate that for short-term predictions (less than 10 minutes into the future), the proposed graph-based approaches outperform state-of-the-art deep learning models, such as long-term short memory, convolutional neuronal networks or hybrid models.},
    address = {Rhodes, Greece},
    author = {Mihaita, Adriana-Simona and Papachatgis, Zac and Rizoiu, Marian-Andrei},
    booktitle = {23rd IEEE International Conference on Intelligent Transportation Systems (ITSC'20)},
    URL_paper = {https://arxiv.org/pdf/2006.14824},
    pages = {1--8},
    title = {{Graph modelling approaches for motorway traffic flow prediction}},
    year = {2020}
}

@inproceedings{Wu2020,
    abstract = {A comprehensive understanding of data bias is the cornerstone of mitigating biases in social media research. This paper presents in-depth measurements of the effects of Twitter data sampling across different timescales and different subjects (entities, networks, and cascades). By constructing two complete tweet streams, we show that Twitter rate limit message is an accurate measure for the volume of missing tweets. Despite sampling rates having clear temporal variations, we find that the Bernoulli process with a uniform rate well approximates Twitter data sampling, and it allows to estimate the ground-truth entity frequency and ranking with the observed sample data. In terms of network analysis, we observe significant structure changes in both the user-hashtag bipartite graph and the retweet network. Finally, we measure the retweet cascades. We identify risks for information diffusion models that rely on tweet inter-arrival times and user influence. This work calls attention to the social data bias caused by data collection, and proposes methods to measure the systematic biases introduced by sampling.},
    archivePrefix = {arXiv},
    arxivId = {2003.09557},
    author = {Wu, Siqi and Rizoiu, Marian-Andrei and Xie, Lexing},
    booktitle = {International AAAI Conference on Web and Social Media (ICWSM '20)},
    eprint = {2003.09557},
    month = {mar},
    pages = {1----10},
    title = {{Variation across Scales: Measurement Fidelity under Twitter Data Sampling}},
    URL_paper = {https://arxiv.org/pdf/2003.09557.pdf},
    URL_code = {https://github.com/avalanchesiqi/twitter-sampling},
    year = {2020}
}

