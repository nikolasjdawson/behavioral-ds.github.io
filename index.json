[{"authors":["rohit-ram"],"categories":null,"content":"","date":1605139200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1605139200,"objectID":"3bc23ee679b3318384aadb6ddcf712af","permalink":"https://www.behavioral-ds.ml/authors/rohit-ram/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/rohit-ram/","section":"authors","summary":"","tags":null,"title":"Rohit Ram","type":"authors"},{"authors":["quyu-kong"],"categories":null,"content":"","date":1573516800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1573516800,"objectID":"ad55124a082344b02dd6b633f167c02c","permalink":"https://www.behavioral-ds.ml/authors/quyu-kong/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/quyu-kong/","section":"authors","summary":"","tags":null,"title":"Quyu Kong","type":"authors"},{"authors":["alexander-soen"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2f1bc0185b27a60a128583d3386e5d82","permalink":"https://www.behavioral-ds.ml/authors/alexander-soen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/alexander-soen/","section":"authors","summary":"","tags":null,"title":"Alexander Soen","type":"authors"},{"authors":["dima-galat"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"ec3e6f1b3b5f9cf5f63d7c94288bf326","permalink":"https://www.behavioral-ds.ml/authors/dima-galat/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/dima-galat/","section":"authors","summary":"","tags":null,"title":"Dima Galat","type":"authors"},{"authors":["kriti-tripathi"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"0e195dcb27b476a5295ec2a6c5b09223","permalink":"https://www.behavioral-ds.ml/authors/kriti-tripathi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/kriti-tripathi/","section":"authors","summary":"","tags":null,"title":"Kriti Tripathi","type":"authors"},{"authors":["ma-rizoiu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"4f906e7173ba13f37fe457c2f8f5aa2a","permalink":"https://www.behavioral-ds.ml/authors/ma-rizoiu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ma-rizoiu/","section":"authors","summary":"","tags":null,"title":"Marian-Andrei Rizoiu","type":"authors"},{"authors":["nik-dawson"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"336eca5812f78357754a398d8d048657","permalink":"https://www.behavioral-ds.ml/authors/nik-dawson/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/nik-dawson/","section":"authors","summary":"","tags":null,"title":"Nik Dawson","type":"authors"},{"authors":["quyu-kong"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"99578dd32d3d54feba0441767c95358d","permalink":"https://www.behavioral-ds.ml/authors/rui-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/rui-zhang/","section":"authors","summary":"","tags":null,"title":"Rui Zhang","type":"authors"},{"authors":["yaozhong-liu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"d4dd97b06e8eb332e7faa76e1de0d262","permalink":"https://www.behavioral-ds.ml/authors/yaozhong-liu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yaozhong-liu/","section":"authors","summary":"","tags":null,"title":"Yaozhong Liu","type":"authors"},{"authors":["Quyu Kong, Rohit Ram"],"categories":["blogpost"],"content":"We demonstrate in this blog post a tutorial on applying the tools for analyzing online information diffusions about Twitter users, birdspotter and evently.\nDataset In this tutorial, we apply two tools for analyzing Twitter users, on a COVID-19 retweet dataset. The dataset is curated by Chen, et al. One can obtain a copy of the tweet IDs from their [project]](https://github.com/echen102/COVID-19-TweetIDs). We only use the 31st of Janury sample of the whole dataset for demonstration purpose. The tweets can be recovered by de-hydrating from their IDs. We note that some tweets might have been deleted and in the end we manage to get 69.2% (1,489,877) of the original tweets.\nTools While BirdSpotter captures the social influence and botness of Twitter users, evently specifically models the temporal dynamics of online information diffusion. We leverage information provided by the tools to study the users in the COVID19 dataset.\nlibrary(evently) library(reticulate) birdspotter \u0026lt;- import('birdspotter')  Preprocessing tweets At this step, we seek to extract diffusion cascades from the COVID-19 dataset for analyzing user influence and botness. A diffusion cascade consist of an initial tweet posted by a Twitter user and followed then by a sereis of retweets. A function provided by evently allows one to obtain cascades from JSON formatted raw tweets. On the other hand, we initialize a BirdSpotter instance and compute the influence and botness scores for all users in the dataset.\ncascades \u0026lt;- parse_raw_tweets_to_cascades('corona_2020_01_31.jsonl', keep_user = T, keep_absolute_time = T) bs \u0026lt;- birdspotter$BirdSpotter('corona_2020_01_31.jsonl') labeled_users \u0026lt;- bs$getLabeledUsers()[, c('user_id', 'botness', 'influence')]  As we cannot publish corona_2020_01_31.jsonl due to Twitter TOC, we have stored the results and load them below\nload('corona_2020_01_31.rda') labeled_users \u0026lt;- read.csv('corona_31_botness_influence.csv', stringsAsFactors = F, colClasses=c(\u0026quot;character\u0026quot;,rep(\u0026quot;numeric\u0026quot;,3)))  We note that all user IDs have been encrypted. After obtaining the results, let’s first conduct some simple measurements on users and cascades.\nlibrary(ggplot2) # check the density of these two values mean_bot \u0026lt;- mean(labeled_users$botness, na.rm = T) ggplot(labeled_users, aes(botness)) + stat_density(geom = 'line') + geom_vline(xintercept = mean_bot, linetype=2, color = 'red') + geom_text(data=data.frame(), aes(x = mean_bot, y = 2, label= sprintf('mean: %s', round(mean_bot, 2))), color= 'red', angle=90, vjust=-0.11)  mean_inf \u0026lt;- mean(labeled_users$influence) ggplot(labeled_users) + stat_ecdf(aes(influence, 1 - ..y..)) + scale_x_log10() + scale_y_log10() + ylab('CCDF') + geom_vline(xintercept = mean_inf, linetype=2, color = 'red') +geom_text(data=data.frame(), aes(x = mean_inf, y = 1e-3, label= sprintf('mean: %s', round(mean_inf, 2))), color= 'red', angle=90, vjust=-0.11)  ## Warning: Transformation introduced infinite values in continuous y-axis  mean_value \u0026lt;- mean(sapply(cascades, nrow)) ggplot(data.frame(size = sapply(cascades, nrow))) + stat_ecdf(aes(size, 1 - ..y..)) + scale_x_log10() + scale_y_log10() + geom_vline(xintercept = mean_value, linetype=2, color = 'red') + geom_text(data=data.frame(), aes(x = mean_value, y = 1e-3, label= sprintf('mean: %s', round(mean_value, 2))), color= 'red', angle=90, vjust=-0.11) + xlab('cascade size') + ylab('CCDF')  ## Warning: Transformation introduced infinite values in continuous y-axis  mean_value2 \u0026lt;- mean(sapply(cascades, function(c) c$time[nrow(c)])) ggplot(data.frame(time = sapply(cascades, function(c) c$time[nrow(c)]))) + stat_ecdf(aes(time, 1 - ..y..)) + scale_x_continuous(trans = 'log1p', breaks = c(0, 100, 10000, 1000000), labels = c('0', '1e2', '1e4', '1e6')) + scale_y_log10() + geom_vline(xintercept = mean_value2, linetype=2, color = 'red') + geom_text(data=data.frame(), aes(x = mean_value2, y = 1e-3, label= sprintf('mean: %s', round(mean_value2, 2))), color= 'red', angle=90, vjust=-0.11) + xlab('cascade final event time')+ ylab('CCDF')  ## Warning: Transformation introduced infinite values in continuous y-axis  mean_value \u0026lt;- mean(labeled_users$activity) ggplot(data.frame(size = labeled_users$activity)) + stat_ecdf(aes(size, 1 - ..y..)) + scale_x_log10() + scale_y_log10() + geom_vline(xintercept = mean_value, linetype=2, color = 'red') + geom_text(data=data.frame(), aes(x = mean_value, y = 1e-3, label= sprintf('mean: %s', round(mean_value, 2))), color= 'red', angle=90, vjust=-0.11) + xlab('user activity')+ ylab('CCDF')  ## Warning: Transformation introduced infinite values in continuous y-axis  Retrain the bot detector If one find the botness scores are not accurate, birdspotter provides a relabeling tool and a retrain API to learn from the given relabeled dataset\n# output a file for mannual labeling bs$getBotAnnotationTemplate('users_to_label.csv') # Once annotated the botness detector can be trained with bs$trainClassifierModel('users_to_label.csv')  Fit user posted cacsades with evently We model a group of cascades initiated by a particular user jointly and treat the fitted model as a characterization of the user. In this example, we select two users for comparison.\nselected_users \u0026lt;- c('369686755237813560', '174266868073402929') # fit Hawkes process on cascades initiated by the selected users user_cascades_fitted \u0026lt;- lapply(selected_users, function(user) { # select cascades that are initiated by the \u0026quot;selected_user\u0026quot; selected_cascades \u0026lt;- Filter(function(cascade) cascade$user[[1]] == user, cascades) # obtain the observation times; # note 1580515200 is 1st Feb when the observation stopped # as we only observed until the end of 31st Jan times \u0026lt;- 1580515200 - sapply(selected_cascades, function(cas) cas$absolute_time[1]) # fit a model on the selected cascades; fit_series(data = selected_cascades, model_type = 'mPL', observation_time = times, cores = 10) }) user_cascades_SEISMIC_fitted \u0026lt;- lapply(selected_users, function(user) { selected_cascades \u0026lt;- Filter(function(cascade) cascade$user[[1]] == user, cascades) times \u0026lt;- 1580515200 - sapply(selected_cascades, function(cas) cas$absolute_time[1]) fit_series(data = selected_cascades, model_type = 'SEISMIC', observation_time = times) }) # check the fitted kernel functions plot_kernel_function(user_cascades_fitted) + scale_color_discrete(labels = c(\u0026quot;@BobOngHugots\u0026quot;, \u0026quot;@Jaefans_Global\u0026quot;))  The plot shows the fitted kernel functions of these two users which reflect their time-decaying influence of attracting followers to reshare their posts. We then demonstrate how to simulate new cascades\nset.seed(134841) user_magnitude \u0026lt;- Filter(function(cascade) cascade$user[[1]] == selected_users[[1]], cascades)[[1]]$magnitude[1] # simulate a new cascade from @BobOngHugots sim_cascade \u0026lt;- generate_series(user_cascades_fitted[[1]], M = user_magnitude) plot_event_series(cascade = sim_cascade, model = user_cascades_fitted[[1]])  selected_cascade \u0026lt;- Filter(function(cascade) cascade$user[1] == selected_users[[1]], cascades)[[1]] selected_time \u0026lt;- user_cascades_fitted[[1]]$observation_time[1] # simulate a cascade with a \u0026quot;selected_cascade\u0026quot; from @BobOngHugots sim_cascade \u0026lt;- generate_series(user_cascades_fitted[[1]], M = user_magnitude, init_history = selected_cascade) sprintf('%s new events simulated after cascade', nrow(sim_cascade[[1]]) - nrow(selected_cascade))  ## [1] \u0026quot;25 new events simulated after cascade\u0026quot;  predict_final_popularity(user_cascades_fitted[[1]], selected_cascade, selected_time)  ## [1] 458.303  # predict with SEISMIC model, assume we have fitted the SEISMIC model predict_final_popularity(user_cascades_SEISMIC_fitted[[1]], selected_cascade, selected_time)  ## [1] 729.923  get_branching_factor(user_cascades_fitted[[1]])  ## [1] 0.7681281  get_viral_score(user_cascades_fitted[[1]])  ## [1] 7.407763  Visualize users in a latent space We show a visualization of top 300 users posted most tweets using the features returned by evently along with the botness and influence scores from birdspotter.\n# obtain observation times here again times \u0026lt;- 1580515200 - sapply(cascades, function(cas) cas$absolute_time[1]) # indicate the grouping of each cascade with the user who started the cascade names(cascades) \u0026lt;- sapply(cascades, function(cas) cas$user[1]) # fit Hawkes processes on all cascades first fitted_corona \u0026lt;- group_fit_series(cascades, model_type = 'mPL', observation_time = times)  The fitting procedure takes quite long so we again load the pre-fitted models here\nload('fitted_models.rda') # choose the top 300 users who started most cacsades selected_users \u0026lt;- labeled_users$user_id[labeled_users$user_id %in% names(sort(sapply(fitted_corona, length), decreasing = T)[seq(300)])] # gather the stats for these users user_influences \u0026lt;- labeled_users$influence[labeled_users$user_id %in% selected_users] user_botness \u0026lt;- labeled_users$botness[labeled_users$user_id %in% selected_users] fitted_corona_selected \u0026lt;- fitted_corona[selected_users] # get the features features \u0026lt;- generate_features(fitted_corona_selected) # compute distances between users using manhattan distance features \u0026lt;- features[, -1] # remove the user id column distances \u0026lt;- dist(features, method = 'manhattan') library(tsne) positions \u0026lt;- tsne(distances, k = 2)  ## sigma summary: Min. : 0.34223375605395 |1st Qu. : 0.457223801885988 |Median : 0.489891425900637 |Mean : 0.500483006369232 |3rd Qu. : 0.538593613780411 |Max. : 0.676779919259545 | ## Epoch: Iteration #100 error is: 14.1961110881254 ## Epoch: Iteration #200 error is: 0.490122133064818 ## Epoch: Iteration #300 error is: 0.474257867010761 ## Epoch: Iteration #400 error is: 0.472067779170087 ## Epoch: Iteration #500 error is: 0.471844181155159 ## Epoch: Iteration #600 error is: 0.471798834134577 ## Epoch: Iteration #700 error is: 0.471783207059971 ## Epoch: Iteration #800 error is: 0.471632929621924 ## Epoch: Iteration #900 error is: 0.47087861882558 ## Epoch: Iteration #1000 error is: 0.470873765976829  df \u0026lt;- data.frame(x = positions[,1], y = positions[,2], influence = user_influences, botness = user_botness) df \u0026lt;- cbind(df, data.frame(botornot = ifelse(df$botness \u0026gt; 0.6, 'Bot', 'Not Bot'))) ggplot(df, aes(x, y, color = influence, shape = botornot, size = botornot)) + geom_point() + scale_shape_manual(values = c(15,1)) + scale_size_manual(values = c(1.5, 1.2)) + scale_color_gradient(low = '#56B1F7', high = '#132B43', trans = 'log10') + theme_void() + labs(size = NULL, shape = NULL) + theme(legend.direction = 'horizontal', legend.position = c(0.8, 0.2), legend.key.size = unit(.3, 'cm'), legend.text = element_text(size = 6), legend.title = element_text(size = 6), legend.spacing = unit(.05, 'cm'))  ","date":1606953600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606953600,"objectID":"f5077c2eb7b2743151221fc6f3881cac","permalink":"https://www.behavioral-ds.ml/blogpost/user_analysis/","publishdate":"2020-12-03T00:00:00Z","relpermalink":"/blogpost/user_analysis/","section":"blogpost","summary":"We demonstrate in this blog post a tutorial on applying the tools for analyzing online information diffusions about Twitter users, birdspotter and evently.\nDataset In this tutorial, we apply two tools for analyzing Twitter users, on a COVID-19 retweet dataset. The dataset is curated by Chen, et al. One can obtain a copy of the tweet IDs from their [project]](https://github.com/echen102/COVID-19-TweetIDs). We only use the 31st of Janury sample of the whole dataset for demonstration purpose.","tags":null,"title":"User Analysis on reshare cascades about COVID-19","type":"blogpost"},{"authors":["Rohit Ram"],"categories":["blogpost"],"content":"   Most statistics students will be familiar with the phrase “correlation isn’t causation,” however, this doesn’t feature strongly in the remainder of their educations. To overcome this hurdle, the researchers’ best practice in experimental design is the randomized controlled trial. However, there are only specific experiments that we can perform. For example, to test the whether smoking causes cancer, we can’t force subjects to smoke. ⊕In the 1950s the tobacco companies argued that there could be some confounding factor (a gene) which smokers and lung cancer patients shared. In general, restricting ourselves to experimental studies to determine causation is incredibly limiting (especially for data scientists). We want to make the same causal conclusions from observational studies and those from experimental studies. We can do that by studying causal inference.\nSimpson’s Paradox An example of the importance of understanding causal relationships is given by Simpson’s Paradox (Simpson 1951), which describes a peculiar phenomenon that can present in data sets, where a correlation between two variables is present in one direction but reverses in each stratum of the data. The paradox expressed best through an example: ⊕This appears to suggest that the more someone exercises, the higher their cholesterol is! This is absurd!  Figure 1: The results of an experiment, where x-axis represents how much exercise an individual does in hours, and y-axis represents cholestral measurment for the same individual.  Figure 1 shows a positive correlation in an experiment that measures individuals’ exercise per week and cholesterol. At first glance, this seems absurd, but when we partition the data by another causal variable, this seems reasonable:  Figure 2: The same results as the experiment above, partioned by age  ⊕(Also note, we have fabricated the data, although these relationships are quite plausible) Understanding the full causal story is essential. Without an entire causal narrative, we might recommend inappropriate interventions; for example, a doctor might prescribe less exercise to reduce cholesterol in the case above.\nTo deduce such causal stories, we need to apply the methodology of causal inference.\n Structural Equation Models and Causal Graphs A structural equation model (SEM) is a set of equations representing the relationship between variables. For example, the equations which generated the data from the Simpson’s paradox example, are given as: \\[ \\begin{align*} age \u0026amp;= U_1 \\\\ exercise \u0026amp;= \\frac{1}{13}*age + U_2 \\\\ cholesteral \u0026amp;= -4*exercise + age + U_3 \\end{align*} \\] We can think of \\(U_1\\), \\(U_2\\), and \\(U_3\\) as specific unobserved exogenous variables of an individual, which generate their endogenous variables (something like error terms).\nA causal graph is a DAG which describes the existence of relationships between variables in a model. An edge x -\u0026gt; y represents the relationship x directly causes y. Consequently, causal graphs can represent SEMs: Indeed this graph shows how age confounds the effect of exercise on cholesterol.\n Do-calculus Pearl (1995) outline a method to remove this confounding (and other similar scenarios) using do-calculus. Outlining the specifics of do-calculus is beyond the scope of this blog post (but for interested readers, we suggest (Pearl, Glymour, and Jewell 2016)). In brief, do-calculus introduces the \\(do()\\) operator, which acts as an intervention and fixes a variable to a particular constant. For example, consider a similar binary situation to the Simpson’s paradox example, where exer is a binary variable true if the individual is active, chol is a binary variable true if the individual has high cholesterol, and age is a binary variable true if the individual is over 60.\nbin_simpsons_data \u0026lt;- simpsons_data %\u0026gt;% mutate(age = age \u0026gt; 60) %\u0026gt;% # Binarize the age, so those over 60 are True, and under 60 are False mutate(exer = exercise\u0026gt;mean(exercise)) %\u0026gt;% # Binarize the exercise level, so those above the average are True, and under are False mutate(chol = cholesteral\u0026gt;mean(cholesteral)) # Binarize the cholesteral level, so those above the average are True, and under are False We ask the same experimental question; does exercise reduce cholesterol. A naive approach would be to compute the effect as \\(P(chol | exer = 1) - P(chol | exer = 0)=\\) 0.168, where \\(P(chol | exer)\\) is computed by filtering the data according to exer. Taking this approach, we would erroneously observe that the effect was positive since those who exercise more are also old and more likely to have high cholesterol.\nThe experimental best practice approach would be to perform a randomized controlled trial (RCT). A random selection of individuals are assigned to do a high exercise regiment and the others do a low exercise regiment (regardless of age). The RCT implicitly removes the natural tendency of exercise to vary with age and allows researchers to observe the causal effect of exercise on cholesterol. When using data generated in such a fashion, increases/decreases in the probability of having high cholesterol caused by exercise are given by \\(P_{RCT}(chol | exer = 1) - P_{RCT}(chol | exer = 0)\\). This metric is known as the Average Causal Effect (ACE), sometimes called the Average Treatment Effect. Note that by conditioning on \\(exer=x\\), with data generated by an RCT, researchers are essentially limiting the data used to estimate \\(P_{RCT}(chol | exer = x)\\), to individuals who were forced to do an exercise regiment \\(x\\). The do here represents forcing individuals to take an intervention value, regardless of their natural tendency, and this is captured by the \\(do()\\) operator. In this case, \\(P(chol | do(exer = x)) = P_{RCT}(chol | exer = x)\\), since the data was generated with an RCT. However, RCTs can be prohibitively expensive (both in time and money) and might not be necessary to tease out a causal effect.\nWe would still like to estimate the ACE, \\(P(chol | do(exer = 1)) - P(chol | do(exer = 0))\\), by using data that wasn’t generated from an RCT. By using the \\(do()\\) operator here, we aim to disassociate exer from its natural tendency with age and effectively perform a graph surgery:\nPearl, Glymour, and Jewell (2016) provide an adjustment formula for just this scenario: \\[ P(y|do(x)) = \\sum_z \\frac{P(X=x, Y=y, PA=z)}{P(X=x| PA=z)} \\] where \\(X\\) represents the variable we are acting on, \\(Y\\) the variable we measure results from, and \\(PA\\) the parents of \\(X\\) and \\(Y\\) or more generally any nodes that satisfy the back-door criterion (which we will introduce later). Note this allows us to derive the causal effect, as if we had generated data with an RCT, using only probabilities estimated from data not generated by an RCT.\nAs such we compute our ACE for the binary scenario:\n# The Joint Distribution P(age, exer, chol) i.e. P(x,y,z) p_aec \u0026lt;- bin_simpsons_data %\u0026gt;% count(age, exer, chol) %\u0026gt;% mutate(freq = n/sum(n)) # The Marginal Distribution P(age) i.e. P(z) p_a \u0026lt;- bin_simpsons_data %\u0026gt;% count(age) %\u0026gt;% mutate(freq = n/sum(n)) # The Marginal Distribution P(age, exer) i.e. P(x, z) p_ea \u0026lt;- bin_simpsons_data %\u0026gt;% count(age, exer) %\u0026gt;% mutate(freq = n/sum(n)) # The Conditional Mariginal Distribution P(exer | age) i.e. P(x | z) p_e_a \u0026lt;- p_a %\u0026gt;% right_join(p_ea, by=\u0026quot;age\u0026quot;) %\u0026gt;% mutate(freq = freq.y/freq.x) %\u0026gt;% select(age, exer, freq) # The Intervention Distribution P(chol | do(exer)) i.e. P(y | do(x)) probabilities \u0026lt;- data.table(p_aec %\u0026gt;% left_join(p_e_a, by=c(\u0026quot;age\u0026quot;, \u0026quot;exer\u0026quot;)) %\u0026gt;% mutate(freq = freq.x/freq.y) %\u0026gt;% select(age, exer, chol, freq) %\u0026gt;% filter(chol) # We are only concerned with what cause high cholestral ) # The average causal effect of exer on chol ACE \u0026lt;- sum(probabilities[exer==T, freq]) - sum(probabilities[exer==F, freq])  This procedure leads to a negative ACE of -0.175, which shows the causal effect of going from high to low exercise on the probability of getting high cholesterol.\nA natural question that follows from this example is, under what conditions can we use such adjustments to achieve an identifiable causal effect.\n d-seperation To understand common scenarios where the effect of variable \\(X\\) on \\(Y\\) is identifiable within a causal graph, we must first introduce the concept of d-separation, also known as blocking. A pair of variable \\(X\\) and \\(Y\\) are said to be blocked if they are conditionally independent, given a set of nodes \\(Z\\). There are three graph types, which are essential for blocking:\nIn the chain scenario, \\(X \\sim Y\\) is blocked by conditioning on \\(Z={M}\\). This is sometimes refered to as the mediation scenario, which we will address further in the front-door criterion.\nIn the fork scenario, \\(X \\sim Y\\) is blocked by conditioning on \\(Z={Z}\\). This is sometimes refered to as the confounder scenario, which is the situation in the simpson’s paradox example.\nFinally, in the collider scenario, \\(X \\sim Y\\) is blocked by not conditioning on \\(Z={M}\\). The idea that \\(X\\) and \\(Y\\), which are independent, to begin with, can become conditionally dependant is unintuitive. One way to think about this is that we are sharing information received from $ Y $ with $ X $ through $ M $ when we condition on $ M $. For a more thorough investigation into this phenomenon, refer to (Pearl, Glymour, and Jewell 2016).\nA path is said to be blocked by \\(Z\\) if it contains a chain or fork with its middle node in \\(Z\\) or a collider with its middle node not in \\(Z\\).\nWe are now ready to introduce the main criteria for which we can perform adjustments.\n The Backdoor Definition 1 (The Backdoor Criterion) A set of nodes \\(Z\\), given a DAG \\(G\\) and a pair of nodes \\((X,Y)\\), is said to satisfy the backdoor criterion if no node in \\(Z\\) is a descendant of \\(X\\), and \\(Z\\) blocks all paths between \\(X\\) and \\(Y\\), which contain arrows into \\(X\\).  If there exists are set of nodes why satisfy the backdoor criterion, then the effect of \\(X\\) on \\(Y\\) is identifiable and given by: \\[ P(y|do(x)) = \\sum_z \\frac{P(X=x, Y=y, Z=z)}{P(X=x| Z=z)} \\]\nThe backdoor criterion stops undue influence through the backdoor paths; it leaves direct paths between \\(X\\) and \\(Y\\), and it blocks spurious paths.\nIt is clear that { age } satisfies these conditions to be a backdoor adjustment set in the example above.\n The Front-door There are notably common scenarios where this doesn’t work. For example, consider a constructed causal mediation situation, as follows: In this case we cannot use the backdoor criterion, to detect the effect of smoking on cancer because tar is a descendant of smoking, and there exists no direct link between smoking and cancer. We must use instead the frontdoor criterion:\nDefinition 2 (The Frontdoor Criterion) A set of nodes \\(Z\\), given a DAG \\(G\\) and a pair of nodes \\((X,Y)\\), is said to satisfy the frontdoor criterion if; \\(Z\\) intercepts all direct paths from \\(X\\) to \\(Y\\), all paths between \\(X\\) and \\(Z\\) are blocked, and all backdoor paths between \\(Y\\) and \\(Z\\) are blocked by \\(X\\).  If there exists are set of nodes \\(Z\\) which satisfy the frontdoor criterion, and \\(P(x, z)\u0026gt;0\\), then the effect of \\(X\\) on \\(Y\\) is identifiable and given by: \\[ P(y|do(x)) = \\sum_z P(z|x) \\sum_{x^\\prime} P(y|x^\\prime, z)P(x^\\prime) \\] In our smoking scenario, we see that by adjusting for tar , we can observe the effect of smoking on cancer.\n Conclusion The above briefly outlines a core motivation for studying causal inference and causal stories. We summarise some of the underlying theory of causal inference and show practical methodology through the frontdoor and backdoor criterion for determining causal effects through entirely observational studies.\nThere are notable aspects of causal inference we have omitted from this taster. The most gaping is the lack of an explanation for the powerful tool of counterfactuals. We have only presented binary examples here (aside from our motivating example); however, perhaps the most common and useful causal inference application is to continuous examples using regression with linear models. Ultimately, we decided this was beyond causal inference taster’s scope and were more deserving of their own articles. Again, for the interested reader, we recommend Pearl, Glymour, and Jewell (2016), which adds links to many other resources.\nPearl, Judea. 1995. “Causal Diagrams for Empirical Research.” Biometrika 82 (4): 669–88.  Pearl, Judea, Madelyn Glymour, and Nicholas P Jewell. 2016. Causal Inference in Statistics: A Primer. John Wiley \u0026amp; Sons.  Simpson, Edward H. 1951. “The Interpretation of Interaction in Contingency Tables.” Journal of the Royal Statistical Society: Series B (Methodological) 13 (2): 238–41.    ","date":1605139200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605139200,"objectID":"c0026882e04e475b642a60f2f012f815","permalink":"https://www.behavioral-ds.ml/blogpost/causal_inference_taster/","publishdate":"2020-11-12T00:00:00Z","relpermalink":"/blogpost/causal_inference_taster/","section":"blogpost","summary":"Most statistics students will be familiar with the phrase “correlation isn’t causation,” however, this doesn’t feature strongly in the remainder of their educations. To overcome this hurdle, the researchers’ best practice in experimental design is the randomized controlled trial. However, there are only specific experiments that we can perform. For example, to test the whether smoking causes cancer, we can’t force subjects to smoke. ⊕In the 1950s the tobacco companies argued that there could be some confounding factor (a gene) which smokers and lung cancer patients shared.","tags":null,"title":"Causal Inference: A basic taster","type":"blogpost"},{"authors":["Quyu Kong"],"categories":["project"],"content":"This is an example project description\n","date":1573516800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573516800,"objectID":"33abb12721d64552d355551625cd27ba","permalink":"https://www.behavioral-ds.ml/researchproject/project1/","publishdate":"2019-11-12T00:00:00Z","relpermalink":"/researchproject/project1/","section":"researchproject","summary":"This is an example project description","tags":["R"],"title":"Epidemic Hawkes: an example project","type":"researchproject"},{"authors":["Quyu Kong"],"categories":["package"],"content":"Introduction This package is designed for simulating and fitting the Hawkes processes and the HawkesN processes with several options of kernel functions. Currently, it assumes univariate processes without background event rates. Prior knowledge about the models is assumed in the following tutorial and please refer to [1] and [2] for details about the models.\nlibrary(evently)  Installation and dependencies Several dependencies (poweRlaw, AMPL, Ipopt) are required for running this package. These dependencies will be installed automatically by R or by following instructions upon package load.\nInstall the package by executing\nif (!require('devtools')) install.packages('devtools') devtools::install_github('behavioral-ds/evently')  Simulating cascades Let’s first simulate 100 event cascades of the Hawkes process with an exponential kernel function (please refer to the Available models for models and their abbreviations in the package) with a given parameter set, . For each simulation, we only simulate until 5 seconds. The resulted cascades are placed in a single list where each cascade is a data.frame.\nset.seed(4) sim_no \u0026lt;- 100 data \u0026lt;- generate_hawkes_event_series(par = c(K = 0.9, theta = 1), model_type = 'EXP', Tmax = 5, sim_no = sim_no) # alternatively, `generate_hawkes_event_series` also accepts a model class object # e.g. # model \u0026lt;- new_hawkes_model(par = c(K = 0.9, theta = 1), model_type = 'EXP') # generate_hawkes_event_series(model = model, Tmax = 5, sim_no = sim_no) head(data[[1]])  ## magnitude time ## 1 1 0.0000000 ## 2 1 0.5941959 ## 3 1 1.4712411 ## 4 1 1.6105430 ## 5 1 1.7855535 ## 6 1 1.8883869  A simulated process is represented by a data.frame where each row is an event. time indicates the event happening time, while magnitude is the event mark information which is always 1 if model_type is an unmarked model. In the context of retweet diffusion cascades, the first row is the original tweet and all following events are its retweets. time records the relative time (in second) of each retweet to the original tweet and magnitude is the follows’ count of the user who retweeted.\nFitting a model on data We can then fit on the cascades simulated in the previous section. After providing the data and model_type, the fitting procedure will spawn 10 AMPL optimization procedures with different parameter inistializations due to the non-convexity of some likelihood functions. Among the 10 fitted model, the one giving the best likelihood value will be returned. To make the fitting procedure faster, we can specify the number of cores to be used for fitting them in parallel.\nfitted_model \u0026lt;- fit_series(data, model_type = 'EXP', observation_time = 5, cores = 10) fitted_model  ## Model: EXP ## No. of cascades: 100 ## init_par ## K 7.92e+00; theta 1.32e+00 ## par ## K 8.51e-01; theta 1.06e+00 ## Neg Log Likelihood: 285.488 ## lower_bound ## K 1.00e-100; theta 1.00e-100 ## upper_bound ## K 1.00e+04; theta 3.00e+02 ## convergence: 0  Available models There are 8 models available so far in this package:\n   Model Abbreviation (model_type) Intensity Function Parameters     Hawkes process with an exponential kernel function EXP  K,theta   Hawkes process with a power-law kernel function PL  K,c,theta   HawkesN process with an exponential kernel function EXPN  K,theta,N   HawkesN process with a power-law kernel function PLN  K,c,theta,N   Marked Hawkes process with an exponential kernel function mEXP  K,beta,theta   Marked Hawkes process with a power-law kernel function mPL  K,beta,c,theta   Marked HawkesN process with an exponential kernel function mEXPN  K,beta,theta,N   Marked HawkesN process with a power-law kernel function mPLN  K,beta,c,theta,N    Acknowledgement The development of this package is supported by the Green Policy grant from the National Security College, Crawford School, ANU.\nLicense Both dataset and code are distributed under the Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0) license. If you require a different license, please contact us at Quyu.Kong@anu.edu.au or Marian-Andrei@rizoiu.eu.\nReference [1] Rizoiu, M. A., Lee, Y., Mishra, S., \u0026amp; Xie, L. (2017, December). Hawkes processes for events in social media. In Frontiers of Multimedia Research (pp. 191-218). Association for Computing Machinery and Morgan \u0026amp; Claypool.\n[2] Rizoiu, M. A., Mishra, S., Kong, Q., Carman, M., \u0026amp; Xie, L. (2018, April). SIR-Hawkes: Linking epidemic models and Hawkes processes to model diffusions in finite populations. In Proceedings of the 2018 World Wide Web Conference (pp. 419-428). International World Wide Web Conferences Steering Committee.\n[3] Mishra, S., Rizoiu, M. A., \u0026amp; Xie, L. (2016, October). Feature driven and point process approaches for popularity prediction. In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management (pp. 1069-1078). ACM.\n[4] Kong, Q., Rizoiu, M. A., \u0026amp; Xie, L. (2019). Modeling Information Cascades with Self-exciting Processes via Generalized Epidemic Models. arXiv preprint arXiv:1910.05451.\n","date":1573516800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573516800,"objectID":"b32db4bbb114e0b6d8e0361c344237f6","permalink":"https://www.behavioral-ds.ml/softwaretool/evently/","publishdate":"2019-11-12T00:00:00Z","relpermalink":"/softwaretool/evently/","section":"softwaretool","summary":"Introduction This package is designed for simulating and fitting the Hawkes processes and the HawkesN processes with several options of kernel functions. Currently, it assumes univariate processes without background event rates. Prior knowledge about the models is assumed in the following tutorial and please refer to [1] and [2] for details about the models.\nlibrary(evently)  Installation and dependencies Several dependencies (poweRlaw, AMPL, Ipopt) are required for running this package. These dependencies will be installed automatically by R or by following instructions upon package load.","tags":["R"],"title":"evently: simulation, fitting of Hawkes processes","type":"softwaretool"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e368e80244db4f7a9c74bc3236074d9e","permalink":"https://www.behavioral-ds.ml/post/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/","section":"","summary":"","tags":null,"title":"Blog","type":"my_widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://www.behavioral-ds.ml/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"Contact","type":"my_widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9eb50f9088083bebcb7e4cf99e22b9ed","permalink":"https://www.behavioral-ds.ml/news/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/news/","section":"","summary":"","tags":null,"title":"News","type":"my_widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://www.behavioral-ds.ml/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"People","type":"my_widget_page"},{"authors":null,"categories":null,"content":"  \n           , , ,  ,                    ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"40ea366a28f9524de71378c3212c5489","permalink":"https://www.behavioral-ds.ml/publication/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/","section":"","summary":"  \n           , , ,  ,                    ","tags":null,"title":"Publications","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f1d044c0738ab9f19347f15c290a71a1","permalink":"https://www.behavioral-ds.ml/research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/research/","section":"","summary":"","tags":null,"title":"Research","type":"my_widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"03fe44d279d38c966381cde2f5f59736","permalink":"https://www.behavioral-ds.ml/tool/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/tool/","section":"","summary":"","tags":null,"title":"Tool","type":"my_widget_page"}]